xquery version "3.1";

<section class="container">
<h2>Why maps?</h2>
<p>I began by attempting to map the points referenced in each story, but as I did so, I realized that nothing digital is ever simple, and nothing I was illustrating added to my analysis. Instead, I created interpretive “Drucker” maps—visualizations that transcend the digital but are still based on data.  In order to better iterate over these images, I used tracing paper and my laptop screen. I used contemporary street maps to lay down a rough scale, then adapted them based on comparison with maps from the historical moment (see figures 3 and 4). Then, I could see things like general proximity easily, thus allowing myself (or the viewer) to better imagine a street-level view. To descend to street-level in Google Maps also gives the viewer a great departure from the bird’s eye: while this functionality would be nice, I did not want to create a digital tool that did this because it would require me to use Google’s non-open-source mapping API.</p>

<p>Each <a href="maps.xql">early story</a> can be traced to the street level easily, based on references in the story. This specificity of place lends itself well to modern digital and physical mapping because of the way location was expressed: near this building, between these two streets, down the street from this pub, etc. The process of digitizing and mapping these places by modern spatial informational standards requires a several levels of mediation. First, I found the spot I thought a place might be on a modern digital map. Next, I used a variety of vintage maps to verify the place. If the geocoordinates matched on the contemporary and georeferenced vintage map, then I recorded the place and its metadata in an XML gazetteer. Places, of course, can contain other places; thus neighborhoods like Hammersmith contain all the data for every place referenced in all the pieces in the collection. In the cases where this process failed, I had to search out other historical data that might lead me to the area being referenced. These resources are included in the bibliography under “Selected Geographic Data Sources.” Occasionally, the data represented in the gazetteer is a best guess. Notes for relative or ambiguous places were used to help keep track of such guesses. The mapping part of this project is meant to be interpretive, and not representative of a historical reality—I intend to remediate the information and present it in a way people at the time may not have imagined. That is not to say that the information is sourced carelessly; I was deliberate about each point in space and often cross-referenced points across multiple maps. That being said, there are streets referenced that I am convinced were fictional: it is difficult to know whether the historical record is failing you or trying to trick you.</p>
<p>The code underlying this project is all TEI-XML. Contact the administrator at gabikeane at pitt.edu for access.</p>

<h2>The Database and Periodical Studies</h2>
<p>Before the rise of the periodical database in the 2000s, popular periodicals were bound as books, so many volumes were available in one place. As Margaret Beetham pointed out in 1990, “the modern reader of nineteenth-century periodicals is, however, confronted with a paradox… Putting several numbers into one bound volume changes all this, not least by suggesting that really the periodical is a kind of book and the numbers are incomplete sections of the whole” (Beetham 23). This perspective is compounded by the reprinting of fictional works that originally appeared in periodicals all at once in collections. Many of us who read the novels of the 19th-century read them all at once. Digital media has helped us mediate this to an extent, with such projects as Dickens’ Journals Online’s Weekly Readings.  From a literary historical perspective, it is nearly impossible to read a piece as a subscriber might.</p>
<p>The notion that reading periodical pieces as part of a book or collection is in the process of being displaced by large databases such as Hathi Trust and Gale Group. The British Newspaper Archive, which is run by the British Library through a company called Find My Past,  is a largely public-facing record database. Find My Past is a DNA ancestry company in the UK; the foreboding trend of selling those interested in family history into biological data collection is a morally difficult. Many academic databases have access to the British Library’s records, but to use the easy, beautiful interface of this specific archive, one must pay roughly 20 p per month.</p>
<p>Periodical research no longer requires one to leave the home institution (though it does still require one to have a home institution), but it does require one to interface with databases. Some databases are quite well-documented, such as Hathi Trust, but some research tasks require technical skills in order to interact with the API (application programming interface).  Larger groups, like Gale, the resource from which most of the items in this collection are taken, claim copyright protection over the images, fail to provide plain text (though they use it to facilitate full-text searching), and sometimes obscure or omit metadata. They offer visualization tools, which although interesting, do not offer proper documentation for what and why they include certain items in each category. Finally, the DJO offers a more bespoke option for those wishing to study something specific. In the case of DJO, the material is relatively regular and presented both as images of collected originals and as plain-text. The project is licensed under Creative Commons By guidelines, which means users can take anything from the project with a proper citation. The creators are easy to contact via email and were forthcoming with their documentation and internal materials.</p>
<p>The field is dominated by large databases, which are scans of either broadsheets or collected books. These databases do not appear to have been designed for humanists or by humanists. They do not anticipate needs of researchers, not because they have not tried, but because that is a nearly impossible task. My collection is a bespoke piece with full-text search and annotation, grounded by but not limited by my research goals. The idea of creating a dedicated project space for these kinds of documents is somewhat antithetical to most digital edition projects, as their subject matter tends to be based on manuscripts, like books, short stories, poems, or collected letters of important people. These kinds of digital editions privilege the authors in a way that simply is not as useful in periodical studies. Katherine Bode’s <a href="https://www.press.umich.edu/8784777/world_of_fiction"><emph>A World of Fiction: Digital Collections and the Future of Literary History</emph></a> argues for a balanced, representational approach to data, rather than a totalizing one: results are skewed “if the literary data analyzed do not effectively represent the historical context we seek to understand. I draw on the theoretical and practical foundations of textual scholarship to constitute what I call a scholarly edition of a literary system: that is, a model of literary works that were published, circulated, and read—and thereby accrued meaning—in a specific historical context, constructed with reference to the history of transmission by which documentary evidence of those works is constituted” (Bode 4). With this project, though it is small, I hope to model the way one may go about subject-focused periodical collections. I think the process of collecting and remediating these texts is a departure from the ways in which we have collected texts, both physically and digitally, since their publication. Whether it is useful to anyone except me, I am unsure, but I think that the process of creating this kind of collection might be useful to others.</p>
</section>
